{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Enhanced Pedestrian Intention Detection for ADAS and Autonomous Vehicles**\n",
        "Training and evaluation of a new model through the use of the PIE dataset."
      ],
      "metadata": {
        "id": "YUe0ZsxDRltG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Step 1: Mount google drive"
      ],
      "metadata": {
        "id": "mNLSy-xsRwRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfwwmsMgZxuJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps 2-4\n",
        "## Step 2: Clone the PIE repositry to the content directory on google colab:\n",
        "https://github.com/aras62/PIE.git\n",
        "\n",
        "##Step 3: Unzip annotations and annotations_vehicle\n",
        "\n",
        "##Step 4: Clone the YOLOP repository into the content directory:\n",
        "https://github.com/hustvl/YOLOP.git"
      ],
      "metadata": {
        "id": "WAm70UiDRyQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjvmLJzhB5A4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/aras62/PIE.git\n",
        "!unzip /content/PIE/annotations/annotations.zip -d /content/PIE\n",
        "!unzip /content/PIE/annotations/annotations_vehicle.zip -d /content/PIE\n",
        "!git clone https://github.com/hustvl/YOLOP.git\n",
        "!mkdir /content/PIE/PIE_clips"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Download set1/set2/set3/set4/set5/set6 video clips from the PIE dataset"
      ],
      "metadata": {
        "id": "bfsahFwFSQtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/PIE/PIE_clips/set01\n",
        "!wget -r --no-parent -P/content/PIE/PIE_clips/set01 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set01/\""
      ],
      "metadata": {
        "id": "FpdGjwSWSnWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/PIE/PIE_clips/set02\n",
        "!wget -r --no-parent -P/content/PIE/PIE_clips/set02 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set02/\""
      ],
      "metadata": {
        "id": "7l39haLjSoQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/PIE/PIE_clips/set03\n",
        "!wget -r --no-parent -P/content/PIE/PIE_clips/set03 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set03/\""
      ],
      "metadata": {
        "id": "jUxdBDiiSooB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/PIE/PIE_clips/set04\n",
        "!wget -r --no-parent -P/content/PIE/PIE_clips/set04 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set04/\""
      ],
      "metadata": {
        "id": "AND9j28_So_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/PIE/PIE_clips/set05\n",
        "!wget -r --no-parent -P/content/PIE/PIE_clips/set05 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set05/\""
      ],
      "metadata": {
        "id": "rfJXfXyeSpUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/PIE/PIE_clips/set06\n",
        "!wget -r --no-parent -P/content/PIE/PIE_clips/set06 -nH -nd \"https://data.nvision2.eecs.yorku.ca/PIE_dataset/PIE_clips/set06/\""
      ],
      "metadata": {
        "id": "rbPTCeiXSprO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Import keras and tensorflow utilities"
      ],
      "metadata": {
        "id": "OOuJk82ZS1lR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-e0IJZyo6r8"
      },
      "outputs": [],
      "source": [
        "#Requires Keras 2.2.5 and Tensorflow 1.15.0\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "#import keras.backend as K\n",
        "#K.image_data_format('channels_last')\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Activation\n",
        "from keras.layers import AveragePooling3D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv3D\n",
        "from keras.layers import Conv3DTranspose\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Add\n",
        "from keras.layers import Multiply\n",
        "from keras.layers import GlobalAveragePooling3D\n",
        "from keras.layers import GlobalMaxPooling3D\n",
        "from keras.layers import Input\n",
        "from keras.layers import MaxPooling3D\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import UpSampling3D\n",
        "from keras.layers import concatenate\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "!sudo pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.layers import SubPixelUpscaling\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import get_source_inputs\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC, F1Score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from PIE.utilities.pie_data import PIE\n",
        "pie = PIE(data_path='/content/PIE')\n",
        "\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEkRoot6btWY"
      },
      "source": [
        "## Step 7: Define a function YOLOPdetect that returns the drivable area and lane segmentation for an image.\n",
        "\n",
        " This is a modified version of the YOLOPdetect function that is in demo.py in the YOLOP directory. All credit goes to its authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzs7fkhgDhci"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os, sys\n",
        "\n",
        "BASE_DIR = os.path.dirname('/content/YOLOP')\n",
        "sys.path.append('/content/YOLOP')\n",
        "\n",
        "import shutil\n",
        "import time\n",
        "from pathlib import Path\n",
        "import imageio\n",
        "\n",
        "print(sys.path)\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "import scipy.special\n",
        "import torchvision.transforms as transforms\n",
        "import PIL.Image as image\n",
        "\n",
        "!sudo pip install yacs\n",
        "from lib.config import cfg\n",
        "from lib.config import update_config\n",
        "from lib.utils.utils import create_logger, select_device, time_synchronized\n",
        "from lib.models import get_net\n",
        "from lib.dataset import LoadImages, LoadStreams\n",
        "from lib.core.general import non_max_suppression, scale_coords\n",
        "from lib.utils import plot_one_box,show_seg_result\n",
        "from lib.core.function import AverageMeter\n",
        "from lib.core.postprocess import morphological_process, connect_lane\n",
        "from lib.utils import letterbox_for_img, clean_str\n",
        "from tqdm import tqdm\n",
        "from numba import cuda\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "\n",
        "transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "class opt:\n",
        "      device = '0'\n",
        "      weights = '/content/YOLOP/weights/End-to-end.pth'\n",
        "      source = '/content/YOLOPstream.png'\n",
        "      img_size = 640\n",
        "      conf_thres = 0.25\n",
        "      iou_thres = 0.45\n",
        "      save_dir = '/content/YOLOP/output'\n",
        "\n",
        "opt = opt()\n",
        "\n",
        "# logger, _, _ = create_logger(\n",
        "#     cfg, cfg.LOG_DIR, 'demo')\n",
        "\n",
        "device = select_device(None,opt.device)\n",
        "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "# Load model\n",
        "yolop_model = get_net(cfg)\n",
        "checkpoint = torch.load(opt.weights, map_location= device)\n",
        "yolop_model.load_state_dict(checkpoint['state_dict'])\n",
        "yolop_model = yolop_model.to(device)\n",
        "if half:\n",
        "    yolop_model.half()  # to FP16\n",
        "\n",
        "img = torch.zeros((1, 3, opt.img_size, opt.img_size), device=device)  # init img\n",
        "_ = yolop_model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
        "yolop_model.eval()\n",
        "\n",
        "def YOLOPdetect(img0):\n",
        "\n",
        "    h0, w0 = img0.shape[:2]\n",
        "    img, ratio, pad = letterbox_for_img(img0, new_shape=opt.img_size, auto=True)\n",
        "    h, w = img.shape[:2]\n",
        "    shapes = (h0, w0), ((h / h0, w / w0), pad)\n",
        "\n",
        "    dataset = [(None, img, img0, None, shapes)]\n",
        "    bs = 1  # batch_size\n",
        "\n",
        "\n",
        "    # Get names and colors\n",
        "    names = yolop_model.module.names if hasattr(yolop_model, 'module') else yolop_model.names\n",
        "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
        "\n",
        "\n",
        "    # Run inference\n",
        "    t0 = time.time()\n",
        "\n",
        "    inf_time = AverageMeter()\n",
        "    nms_time = AverageMeter()\n",
        "\n",
        "\n",
        "\n",
        "    da_seg_mask = None\n",
        "\n",
        "    for i, (path, img, img_det, vid_cap,shapes) in enumerate(dataset):\n",
        "        with torch.no_grad():\n",
        "          torch.cuda.empty_cache()\n",
        "          img = transform(img).to(device)\n",
        "          img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "          if img.ndimension() == 3:\n",
        "              img = img.unsqueeze(0)\n",
        "          # Inference\n",
        "          t1 = time_synchronized()\n",
        "          det_out, da_seg_out,ll_seg_out= yolop_model(img)\n",
        "          t2 = time_synchronized()\n",
        "          # if i == 0:\n",
        "          #     print(det_out)\n",
        "          inf_out, _ = det_out\n",
        "          inf_time.update(t2-t1,img.size(0))\n",
        "\n",
        "          # Apply NMS\n",
        "          t3 = time_synchronized()\n",
        "          det_pred = non_max_suppression(inf_out, conf_thres=opt.conf_thres, iou_thres=opt.iou_thres, classes=None, agnostic=False)\n",
        "          t4 = time_synchronized()\n",
        "\n",
        "          nms_time.update(t4-t3,img.size(0))\n",
        "          det=det_pred[0]\n",
        "\n",
        "          _, _, height, width = img.shape\n",
        "          h,w,_=img_det.shape\n",
        "          pad_w, pad_h = shapes[1][1]\n",
        "          pad_w = int(pad_w)\n",
        "          pad_h = int(pad_h)\n",
        "          ratio = shapes[1][0][1]\n",
        "\n",
        "          da_predict = da_seg_out[:, :, pad_h:(height-pad_h),pad_w:(width-pad_w)]\n",
        "          da_seg_mask = torch.nn.functional.interpolate(da_predict, scale_factor=int(1/ratio), mode='bilinear')\n",
        "          _, da_seg_mask = torch.max(da_seg_mask, 1)\n",
        "          da_seg_mask = da_seg_mask.int().squeeze().cpu().numpy()\n",
        "          # da_seg_mask = morphological_process(da_seg_mask, kernel_size=7)\n",
        "\n",
        "\n",
        "          ll_predict = ll_seg_out[:, :,pad_h:(height-pad_h),pad_w:(width-pad_w)]\n",
        "          ll_seg_mask = torch.nn.functional.interpolate(ll_predict, scale_factor=int(1/ratio), mode='bilinear')\n",
        "          _, ll_seg_mask = torch.max(ll_seg_mask, 1)\n",
        "          ll_seg_mask = ll_seg_mask.int().squeeze().cpu().numpy()\n",
        "          # Lane line post-processing\n",
        "          #ll_seg_mask = morphological_process(ll_seg_mask, kernel_size=7, func_type=cv2.MORPH_OPEN)\n",
        "          #ll_seg_mask = connect_lane(ll_seg_mask)\n",
        "\n",
        "          img_det = show_seg_result(img_det, (da_seg_mask, ll_seg_mask), _, _, is_demo=True)\n",
        "\n",
        "          del ll_predict\n",
        "          del da_predict\n",
        "          del ll_seg_mask\n",
        "          del da_seg_mask\n",
        "          del inf_out\n",
        "          del det_out\n",
        "          del det_pred\n",
        "          del img\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "    img_det = cv2.resize(img_det, (1920,1080))\n",
        "    return img_det"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG0q3JQb5Qa4"
      },
      "source": [
        "##Step 7: Setup posenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBPtli6ljiok"
      },
      "outputs": [],
      "source": [
        "# Create a new directory and initialize Git\n",
        "!mkdir posenet\n",
        "!cd posenet\n",
        "!git init\n",
        "\n",
        "# Add the remote repository\n",
        "!git remote add origin https://github.com/michellelychan/posenet-pytorch.git\n",
        "\n",
        "# Enable sparse checkout\n",
        "!git config core.sparseCheckout true\n",
        "\n",
        "# Specify the folder to clone\n",
        "!echo \"/posenet\" >> .git/info/sparse-checkout\n",
        "\n",
        "# Pull the specified folder from the main branch\n",
        "!git pull origin master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF5wDOdDlP34"
      },
      "outputs": [],
      "source": [
        "import posenet\n",
        "from posenet.decode_multi import decode_multiple_poses\n",
        "from posenet.decode import decode_pose\n",
        "pose_model = posenet.load_model(101)\n",
        "pose_model = pose_model.cuda()\n",
        "output_stride = pose_model.output_stride\n",
        "\n",
        "def process_input(source_img, scale_factor=1.0, output_stride=16):\n",
        "    target_width, target_height = posenet.valid_resolution(\n",
        "        source_img.shape[1] * scale_factor, source_img.shape[0] * scale_factor, output_stride=output_stride)\n",
        "    scale = np.array([source_img.shape[0] / target_height, source_img.shape[1] / target_width])\n",
        "    input_img = cv2.resize(source_img, (target_width, target_height), interpolation=cv2.INTER_LINEAR)\n",
        "    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    input_img = input_img * (2.0 / 255.0) - 1.0\n",
        "    input_img = input_img.transpose((2, 0, 1)).reshape(1, 3, target_height, target_width)\n",
        "    return input_img, source_img, scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arbKwYbmkxVZ"
      },
      "source": [
        "##Step 8: Setup the DenseNet model through keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eitiVmxdCx8-"
      },
      "outputs": [],
      "source": [
        "from keras.layers import GRU, concatenate\n",
        "def DenseNet3D_3(input_shape_image, input_shape_pose, input_shape_speed, input_shape_action, input_shape_bbox, num_classes):\n",
        "    # Image input\n",
        "    image_input = Input(shape=input_shape_image, name='image_input')\n",
        "\n",
        "    # Initial Convolution Layer\n",
        "    x = Conv3D(64, (3, 3, 3), padding='same')(image_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling3D(pool_size=(2, 2, 2))(x)\n",
        "\n",
        "    # Dense Block 1\n",
        "    x = Conv3D(128, (3, 3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling3D(pool_size=(2, 2, 2))(x)\n",
        "\n",
        "    # Dense Block 2\n",
        "    x = Conv3D(256, (3, 3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling3D(pool_size=(2, 2, 2))(x)\n",
        "\n",
        "    # Dense Block 3\n",
        "    x = Conv3D(512, (3, 3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = GlobalAveragePooling3D()(x)\n",
        "\n",
        "    # Ensure image branch has a fixed output size\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    # Pose input\n",
        "    pose_input = Input(shape=input_shape_pose, name='pose_input')\n",
        "\n",
        "    # GRU Layer for Pose\n",
        "    pose_x = GRU(64, return_sequences=False)(pose_input)\n",
        "\n",
        "    # Ensure pose branch has the same output size\n",
        "    pose_x = Dense(128, activation='relu')(pose_x)\n",
        "\n",
        "    # Speed input\n",
        "    speed_input = Input(shape=input_shape_speed, name='speed_input')\n",
        "\n",
        "    # GRU Layer for Speed\n",
        "    speed_x = GRU(32, return_sequences=False)(speed_input)\n",
        "\n",
        "    # Ensure speed branch has the same output size\n",
        "    speed_x = Dense(128, activation='relu')(speed_x)\n",
        "\n",
        "    # Action input\n",
        "    action_input = Input(shape=input_shape_action, name='action_input')\n",
        "\n",
        "    # GRU Layer for Action\n",
        "    action_x = GRU(32, return_sequences=False)(action_input)\n",
        "\n",
        "    # Dense layer for Action\n",
        "    action_x = Dense(128, activation='relu')(action_x)\n",
        "\n",
        "    # Bounding Box input\n",
        "    bbox_input = Input(shape=input_shape_bbox, name='bbox_input')\n",
        "\n",
        "    # GRU Layer for Bounding Box\n",
        "    bbox_x = GRU(32, return_sequences=False)(bbox_input)\n",
        "\n",
        "    # Dense layer for Bounding Box\n",
        "    bbox_x = Dense(128, activation='relu')(bbox_x)\n",
        "\n",
        "    # Learnable weights for each input\n",
        "    image_weight = Dense(1, activation='sigmoid')(x)\n",
        "    pose_weight = Dense(1, activation='sigmoid')(pose_x)\n",
        "    speed_weight = Dense(1, activation='sigmoid')(speed_x)\n",
        "    action_weight = Dense(1, activation='sigmoid')(action_x)\n",
        "    bbox_weight = Dense(1, activation='sigmoid')(bbox_x)\n",
        "\n",
        "    # Multiply inputs by their respective weights\n",
        "    weighted_image = Multiply()([x, image_weight])\n",
        "    weighted_pose = Multiply()([pose_x, pose_weight])\n",
        "    weighted_speed = Multiply()([speed_x, speed_weight])\n",
        "    weighted_action = Multiply()([action_x, action_weight])\n",
        "    weighted_bbox = Multiply()([bbox_x, bbox_weight])\n",
        "\n",
        "    # Combine weighted inputs with an addition (weighted average)\n",
        "    combined = Add()([weighted_image, weighted_pose, weighted_speed, weighted_action, weighted_bbox])\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Dense(256, activation='relu')(combined)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[image_input, pose_input, speed_input, action_input, bbox_input], outputs=outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FbUhsflk4bz"
      },
      "source": [
        "## Step 9: Define a function parse_video that returns 100x100x16x3 images, 16x34 pose, 16x2 vehicle speed, 16x2 ped action and 16x4 bbox coords for each pedestrian in each frame of a video from the PIE dataset\n",
        "\n",
        "The input to the function is the ID of the set and ID of the video (eg. 'set01','video_0001')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65wkKP2mk8Bz"
      },
      "outputs": [],
      "source": [
        "def parse_video(set_id,video_id):\n",
        "    rolling_data = {}\n",
        "    rolling_data_pose = {}\n",
        "    rolling_data_speed = {}\n",
        "    rolling_data_action = {}\n",
        "    rolling_data_bbox = {}\n",
        "    annotations = pie._get_annotations(set_id,video_id)\n",
        "    annotations = annotations[\"ped_annotations\"]\n",
        "    vehicle_annotations = pie._get_vehicle_attributes(set_id,video_id)\n",
        "    print(vehicle_annotations)\n",
        "    #vehicle_annotations = vehicle_annotations[\"vehicle_annotations\"]\n",
        "    #video_bboxes = tf.data.Dataset.from_tensor_slices(np.empty((0,100,100,16,3)).astype('uint8'))\n",
        "    #video_labels = tf.data.Dataset.from_tensor_slices(np.empty((0,2)).astype('float32'))\n",
        "    video_bboxes = []\n",
        "    video_speed = []\n",
        "    video_poses = []\n",
        "    video_labels = []\n",
        "    video_action = []\n",
        "    video_bbox_coords = []\n",
        "    video = cv2.VideoCapture(f\"/content/PIE/PIE_clips/{set_id}/{video_id}.mp4\")\n",
        "    for key in annotations.keys():\n",
        "\n",
        "        if(annotations[key][\"behavior\"]=={}):\n",
        "            continue\n",
        "        bboxes = annotations[key][\"bbox\"]\n",
        "        frames = annotations[key][\"frames\"]\n",
        "        labels = annotations[key][\"behavior\"][\"cross\"]\n",
        "        actions = annotations[key][\"behavior\"][\"action\"]\n",
        "        cropped_imgs = []\n",
        "        poses = []\n",
        "        speed = []\n",
        "        action_list = []\n",
        "        bbox_coords = []\n",
        "        video.set(cv2.CAP_PROP_POS_FRAMES, frames[0]-1)\n",
        "        for i, frame_id in enumerate(frames):\n",
        "\n",
        "            ret, frame = video.read()\n",
        "\n",
        "            frame = YOLOPdetect(frame)\n",
        "\n",
        "            bbox = np.array(bboxes[i])\n",
        "            bbox = bbox.astype('int')\n",
        "            x1 = bbox[0] - 10 if bbox[0]  - 10 > 0 else 0\n",
        "            y1 = bbox[1] - 10 if bbox[1]  - 10 > 0 else 0\n",
        "            x2 = bbox[2] + 10 if bbox[2]  + 10 < frame.shape[1] else frame.shape[1]\n",
        "            y2 = bbox[3] + 10 if bbox[3]  + 10 < frame.shape[0] else frame.shape[0]\n",
        "            bbox = [x1, y1, x2, y2]\n",
        "            cropped_img = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
        "\n",
        "            # media_predictions = model_pose.predict(cropped_img, conf=0.2, skip_image_resizing=True)\n",
        "            # poses= media_predictions.prediction.poses\n",
        "            # conf = media_predictions.prediction.scores\n",
        "            # edge_links= media_predictions.prediction.edge_links\n",
        "            # edge_colors=media_predictions.prediction.edge_colors\n",
        "            # keypoint_colors=media_predictions.prediction.keypoint_colors\n",
        "\n",
        "            # clear_output()\n",
        "\n",
        "            #for i in range(len(poses)):\n",
        "            #    cropped_img = draw_skeleton(cropped_img,poses[i],conf[i],edge_links,edge_colors,2,keypoint_colors,4,0.0,False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "              input_image, _, _ = process_input(cropped_img)\n",
        "              input_image = torch.Tensor(input_image).cuda()\n",
        "              heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = pose_model(input_image)\n",
        "              pose_scores, keypoint_scores, keypoint_coords, pose_offsets = decode_multiple_poses(\n",
        "                  heatmaps_result.squeeze(0),\n",
        "                  offsets_result.squeeze(0),\n",
        "                  displacement_fwd_result.squeeze(0),\n",
        "                  displacement_bwd_result.squeeze(0),\n",
        "                  output_stride=output_stride,\n",
        "                  max_pose_detections=1,\n",
        "                  min_pose_score=0.1)\n",
        "\n",
        "            cropped_img = cv2.resize(cropped_img, (100,100))\n",
        "\n",
        "            if key not in list(rolling_data.keys()):\n",
        "                rolling_data[key] = [np.asarray(cropped_img)]\n",
        "            elif len(rolling_data[key]) < 16: # bboxes values for 16 frames\n",
        "               rolling_data[key].append(np.asarray(cropped_img)) # append the image\n",
        "            else:\n",
        "               del rolling_data[key][0] # delete oldest frame bbox and append latest frame bbox\n",
        "               rolling_data[key].append(np.asarray(cropped_img))\n",
        "\n",
        "            keypoint_coords = np.reshape(keypoint_coords[0],(34))\n",
        "\n",
        "            if key not in list(rolling_data_pose.keys()):\n",
        "                rolling_data_pose[key] = [np.asarray(keypoint_coords)]\n",
        "            elif len(rolling_data_pose[key]) < 16: # pose values for 16 frames\n",
        "               rolling_data_pose[key].append(np.asarray(keypoint_coords)) # append the pose\n",
        "            else:\n",
        "               del rolling_data_pose[key][0] # delete oldest frame bbox and append latest frame bbox\n",
        "               rolling_data_pose[key].append(np.asarray(keypoint_coords))\n",
        "\n",
        "            OBD_speed = np.asarray([vehicle_annotations[frame_id][\"OBD_speed\"]]*2)\n",
        "\n",
        "            if key not in list(rolling_data_speed.keys()):\n",
        "                rolling_data_speed[key] = [OBD_speed]\n",
        "            elif len(rolling_data_speed[key]) < 16: # speed values for 16 frames\n",
        "               rolling_data_speed[key].append(OBD_speed) # append the speed\n",
        "            else:\n",
        "               del rolling_data_speed[key][0] # delete oldest frame speed and append latest frame speed\n",
        "               rolling_data_speed[key].append(np.asarray(OBD_speed))\n",
        "\n",
        "            action = np.asarray([actions[i]]*2)\n",
        "\n",
        "            if key not in list(rolling_data_action.keys()):\n",
        "                rolling_data_action[key] = [action]\n",
        "            elif len(rolling_data_action[key]) < 16: # speed values for 16 frames\n",
        "               rolling_data_action[key].append(action) # append the speed\n",
        "            else:\n",
        "               del rolling_data_action[key][0] # delete oldest frame speed and append latest frame speed\n",
        "               rolling_data_action[key].append(np.asarray(action))\n",
        "\n",
        "            bbox_coord = np.asarray(bboxes[i])\n",
        "\n",
        "            if key not in list(rolling_data_bbox.keys()):\n",
        "                rolling_data_bbox[key] = [bbox_coord]\n",
        "            elif len(rolling_data_bbox[key]) < 16: # speed values for 16 frames\n",
        "               rolling_data_bbox[key].append(bbox_coord) # append the speed\n",
        "            else:\n",
        "               del rolling_data_bbox[key][0] # delete oldest frame speed and append latest frame speed\n",
        "               rolling_data_bbox[key].append(np.asarray(bbox_coord))\n",
        "\n",
        "            if(i%2!=0):\n",
        "              continue\n",
        "            if len(rolling_data[key]) == 16:\n",
        "              seq = np.stack(np.array(rolling_data[key]),axis=2) # (100*100*16*3)\n",
        "              #seq = np.expand_dims(seq,axis=0)\n",
        "              cropped_imgs = cropped_imgs+[seq] # classification output\n",
        "            else:\n",
        "              seq = np.stack(np.array([rolling_data[key][-1]] * 16),axis=2)\n",
        "              #seq = np.expand_dims(seq,axis=0)\n",
        "              cropped_imgs = cropped_imgs+[seq]\n",
        "\n",
        "            if len(rolling_data_pose[key]) == 16:\n",
        "              seq = np.stack(np.array(rolling_data_pose[key]),axis=0) # (16*17*2)\n",
        "              poses = poses+[seq] # classification output\n",
        "            else:\n",
        "              seq = np.stack(np.array([rolling_data_pose[key][-1]] * 16),axis=0)\n",
        "              poses = poses+[seq]\n",
        "\n",
        "            if len(rolling_data_speed[key]) == 16:\n",
        "              seq = np.stack(np.array(rolling_data_speed[key]),axis=0) # (16*1)\n",
        "              speed = speed+[seq] # classification output\n",
        "            else:\n",
        "              seq = np.stack(np.array([rolling_data_speed[key][-1]] * 16),axis=0)\n",
        "              speed = speed+[seq]\n",
        "\n",
        "            if len(rolling_data_action[key]) == 16:\n",
        "              seq = np.stack(np.array(rolling_data_action[key]),axis=0) # (16*1)\n",
        "              action_list = action_list+[seq] # classification output\n",
        "            else:\n",
        "              seq = np.stack(np.array([rolling_data_action[key][-1]] * 16),axis=0)\n",
        "              action_list = action_list+[seq]\n",
        "\n",
        "            if len(rolling_data_bbox[key]) == 16:\n",
        "              seq = np.stack(np.array(rolling_data_bbox[key]),axis=0) # (16*1)\n",
        "              bbox_coords = bbox_coords+[seq] # classification output\n",
        "            else:\n",
        "              seq = np.stack(np.array([rolling_data_bbox[key][-1]] * 16),axis=0)\n",
        "              bbox_coords = bbox_coords+[seq]\n",
        "\n",
        "        video_bboxes = video_bboxes + cropped_imgs\n",
        "        video_poses = video_poses + poses\n",
        "        video_speed = video_speed + speed\n",
        "        video_action = video_action + action_list\n",
        "        video_bbox_coords = video_bbox_coords + bbox_coords\n",
        "\n",
        "        labels_ = []\n",
        "        for i in range(len(labels)):\n",
        "          if(i%2==0):\n",
        "            if(labels[i]==-1):\n",
        "              labels_.append(1)\n",
        "            else:\n",
        "              labels_.append(labels[i])\n",
        "        labels = labels_\n",
        "        video_labels = video_labels + labels\n",
        "\n",
        "    print(f\"parsed {video_id}\", flush=True)\n",
        "\n",
        "    return video_bboxes, video_labels, video_poses, video_speed, video_action, video_bbox_coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDt2h0yYk92x"
      },
      "source": [
        "## Step 10: Compile the model\n",
        "* 100x100x16x3 input (16 consecutive 100x100 images of pedestrians)\n",
        "* 16x34 pose input\n",
        "* 16x2 speed input\n",
        "* 16x2 action input\n",
        "* 16x4 BBox coords input\n",
        "* Evaluation metrics for accuracy, precision, recall, AUC and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJU-Xxf2z5EA"
      },
      "outputs": [],
      "source": [
        "model = DenseNet3D_3((100, 100, 16, 3), (16, 34), (16, 2), (16, 2), (16, 4), 2)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.categorical_crossentropy,\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc'), F1Score(name='f1score')]\n",
        ")\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run to train the model\n",
        "Set *curr_set* and *curr_video* to desired starting set and video.\n",
        "This trains the model video by video till the end of the set to save RAM."
      ],
      "metadata": {
        "id": "4o2VvpCqUUjJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NuEPYVF9GX5n"
      },
      "outputs": [],
      "source": [
        "video_num_per_set = [4, 3, 19, 16, 2, 9]\n",
        "curr_set = 1\n",
        "curr_video = 1\n",
        "\n",
        "for i in range(curr_video,video_num_per_set[curr_set-1]+1):\n",
        "\n",
        "  if i!=curr_video:\n",
        "    del X_train, X_val, X_train2, X_val2, X_train3, X_val3, y_train, y_val, dataset_bboxes, dataset_labels, dataset_poses, dataset_speed\n",
        "  gc.collect()\n",
        "\n",
        "  dataset_bboxes = []\n",
        "  dataset_labels = []\n",
        "  dataset_poses = []\n",
        "  dataset_speed = []\n",
        "\n",
        "  video_id = f\"video_000{i}\" if i < 10 else (f\"video_00{i}\" if i < 100 else f\"video_0{i}\")\n",
        "  video_bboxes, video_labels, video_poses, video_speed, dataset_actions, dataset_bbox_coords = parse_video(f'set0{curr_set}', video_id)\n",
        "  dataset_bboxes = dataset_bboxes + video_bboxes\n",
        "  dataset_labels = dataset_labels + video_labels\n",
        "  dataset_poses = dataset_poses + video_poses\n",
        "  dataset_speed = dataset_speed + video_speed\n",
        "\n",
        "  del video_bboxes, video_labels, video_poses, video_speed\n",
        "  gc.collect()\n",
        "\n",
        "  print(dataset_labels)\n",
        "\n",
        "  X_train, X_val, y_train, y_val, X_train2, X_val2, X_train3, X_val3, X_train4, X_val4, X_train5, X_val5 = train_test_split(np.array(dataset_bboxes), np.array(dataset_labels), np.array(dataset_poses), np.array(dataset_speed), np.array(dataset_actions), np.array(dataset_bbox_coords), test_size=0.2, random_state=42)\n",
        "\n",
        "  y_train=tf.one_hot(y_train,depth=2)\n",
        "  y_val=tf.one_hot(y_val,depth=2)\n",
        "\n",
        "  print(X_train.shape)\n",
        "  print(X_train2.shape)\n",
        "  print(X_train3.shape)\n",
        "  print(X_train4.shape)\n",
        "  print(X_train5.shape)\n",
        "\n",
        "  print(y_train.shape)\n",
        "\n",
        "  # Add a checkpoint callback to store the checkpoint that has the highest\n",
        "  # validation accuracy.\n",
        "  checkpoint_path = f\"weights{curr_set}v{i}.best.hdf5.keras\"\n",
        "  checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                              monitor='val_accuracy',\n",
        "                              verbose=1,\n",
        "                              save_best_only=True,\n",
        "                              mode='max')\n",
        "  earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                patience=20)\n",
        "\n",
        "  # Train the model\n",
        "  history = model.fit([X_train, X_train2, X_train3, X_train4, X_train5], y_train,\n",
        "                      batch_size=16,\n",
        "                      epochs=20,\n",
        "                      validation_data=([X_val, X_val2, X_val3, X_val4, X_val5], y_val),\n",
        "                      callbacks=[checkpoint, earlystopping])\n",
        "\n",
        "  # Save the model architecture to JSON file\n",
        "  model_json = model.to_json()\n",
        "  with open(f\"densenet_model_11 (s{curr_set}v{i}).json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "\n",
        "  # Save the model weights to HDF5 file\n",
        "  model.save_weights(f\"densenet_weights_11 (s{curr_set}v{i}).weights.h5\")\n",
        "\n",
        "  # Save the model architecture to JSON file\n",
        "  model_json = model.to_json()\n",
        "  with open(f\"/content/drive/My Drive/densenet_model_11 (s{curr_set}v{i}).json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "\n",
        "  # Save the model weights to HDF5 file\n",
        "  model.save_weights(f\"/content/drive/My Drive/densenet_weights_11 (s{curr_set}v{i}).weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run to evaluate the model on the 6th set"
      ],
      "metadata": {
        "id": "TpsdZQVeVDME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aIpJG1YNMFEF"
      },
      "outputs": [],
      "source": [
        "total_cm = np.array([[0,0],[0,0]])\n",
        "for j in range (1,10):\n",
        "  if(j!=1):\n",
        "    del X_test, X_test2, X_test3, X_test4, X_test5, y_test, dataset_bboxes, dataset_labels, dataset_poses, dataset_speed, dataset_action, dataset_bbox_coords\n",
        "    gc.collect()\n",
        "\n",
        "  dataset_bboxes = []\n",
        "  dataset_labels = []\n",
        "  dataset_poses = []\n",
        "  dataset_speed = []\n",
        "\n",
        "  for i in range (j,j+1):\n",
        "\n",
        "      video_id = f\"video_000{i}\" if i < 10 else (f\"video_00{i}\" if i < 100 else f\"video_0{i}\")\n",
        "      video_bboxes, video_labels, video_poses, video_speed, dataset_action, dataset_bbox_coords = parse_video('set06', video_id)\n",
        "      dataset_bboxes = dataset_bboxes + video_bboxes\n",
        "      dataset_labels = dataset_labels + video_labels\n",
        "      dataset_poses = dataset_poses + video_poses\n",
        "      dataset_speed = dataset_speed + video_speed\n",
        "\n",
        "      del video_bboxes, video_labels, video_poses, video_speed\n",
        "      gc.collect()\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  # Evaluate the model\n",
        "  X_test = np.array(dataset_bboxes)\n",
        "  X_test2 = np.array(dataset_poses)\n",
        "  X_test3 = np.array(dataset_speed)\n",
        "  X_test4 = np.array(dataset_action)\n",
        "  X_test5 = np.array(dataset_bbox_coords)\n",
        "  y_test = tf.one_hot(np.array(dataset_labels),depth=2)\n",
        "  results = model.evaluate([X_test,X_test2,X_test3,X_test4,X_test5], y_test , verbose=1)\n",
        "  print(\"Test Loss: {}, Test Accuracy: {}, Test Precision: {}, Test Recall: {}, Test AUC: {}\".format(results[0], results[1], results[2], results[3], results[4]))\n",
        "\n",
        "  # Open a file for writing ('w' mode), creates the file if it does not exist\n",
        "  with open(f'/content/drive/My Drive/pie model 11 accuracy {j}.txt', 'w') as file:\n",
        "  # Write some text to the file\n",
        "    file.write(\"Test Loss: {}, Test Accuracy: {}, Test Precision: {}, Test Recall: {}, Test AUC: {}\".format(results[0], results[1], results[2], results[3], results[4]))\n",
        "    file.write(\"\\nF1 Score: {}\\n\".format(results[5]))\n",
        "\n",
        "  # Predict probabilities for test set\n",
        "  y_probs = []\n",
        "  y_probs = model.predict([X_test,X_test2,X_test3,X_test4,X_test5])  # these are probabilities of the positive class\n",
        "  y_pred = (np.array(y_probs) > 0.5).astype('int32')  # convert probabilities to binary predictions\n",
        "  y_pred = [x[1] for x in y_pred]\n",
        "\n",
        "  for i in range(len(dataset_labels)):\n",
        "    if(dataset_labels[i]==-1):\n",
        "      dataset_labels[i]=1\n",
        "\n",
        "  # Calculate F1 Score\n",
        "  print(y_pred)\n",
        "  print(dataset_labels)\n",
        "  f1 = f1_score(dataset_labels, y_pred)\n",
        "  print(\"F1 Score:\", f1)\n",
        "\n",
        "  from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "\n",
        "  cm = confusion_matrix(dataset_labels, y_pred)\n",
        "  print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "  total_cm = np.add(total_cm,cm)\n",
        "\n",
        "  accuracy = accuracy_score(dataset_labels, y_pred)\n",
        "  precision = precision_score(dataset_labels, y_pred, zero_division=1)\n",
        "  recall = recall_score(dataset_labels, y_pred, zero_division=1)\n",
        "\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"Recall:\", recall)\n",
        "\n",
        "  # Open a file for writing ('w' mode), creates the file if it does not exist\n",
        "  with open(f'/content/drive/My Drive/pie model 11 accuracy {j}.txt', 'w') as file:\n",
        "  # Write some text to the file\n",
        "    file.write(\"Test Loss: {}, Test Accuracy: {}, Test Precision: {}, Test Recall: {}, Test AUC: {}\".format(results[0], results[1], results[2], results[3], results[4]))\n",
        "    file.write(\"\\nF1 Score: {}\\n\".format(results[5]))\n",
        "    file.write(\"Confusion Matrix:\\n{}\\n\".format(cm))\n",
        "    file.write(\"Accuracy: {}\\n\".format(accuracy))\n",
        "    file.write(\"Precision: {}\\n\".format(precision))\n",
        "    file.write(\"Recall: {}\\n\".format(recall))\n",
        "    file.write(\"F1 Score: {}\\n\".format(f1))\n",
        "    file.write(\"Total Confusion Matrix:\\n{}\\n\".format(total_cm))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kEkRoot6btWY",
        "S-ovBkEp5MNK",
        "NG0q3JQb5Qa4",
        "arbKwYbmkxVZ",
        "2FbUhsflk4bz"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}